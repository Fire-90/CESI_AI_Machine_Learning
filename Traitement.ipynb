{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def process_hr_data():\n",
    "    # 1. Chargement des données\n",
    "    print(\"Chargement des fichiers...\")\n",
    "    general_data = pd.read_csv('data/general_data.csv')\n",
    "    manager_survey = pd.read_csv('data/manager_survey_data.csv')\n",
    "    employee_survey = pd.read_csv('data/employee_survey_data.csv')\n",
    "    in_time = pd.read_csv('data/in_time.csv')\n",
    "    out_time = pd.read_csv('data/out_time.csv')\n",
    "    # 2. Fusion des données principales\n",
    "    # On utilise EmployeeID comme clé de jointure\n",
    "    df_main = general_data.merge(manager_survey, on='EmployeeID', how='left')\n",
    "    df_main = df_main.merge(employee_survey, on='EmployeeID', how='left')\n",
    "\n",
    "    # 3. Traitement des fichiers Temps (Badgeuse)\n",
    "    # Renommer la première colonne vide ou index en 'EmployeeID'\n",
    "    in_time.rename(columns={in_time.columns[0]: 'EmployeeID'}, inplace=True)\n",
    "    out_time.rename(columns={out_time.columns[0]: 'EmployeeID'}, inplace=True)\n",
    "\n",
    "    # Définir l'index pour faciliter les calculs matriciels\n",
    "    in_time.set_index('EmployeeID', inplace=True)\n",
    "    out_time.set_index('EmployeeID', inplace=True)\n",
    "\n",
    "    print(\"Traitement des horaires (conversion et calcul)...\")\n",
    "    # Conversion en datetime (ignorer les erreurs pour les jours fériés/absences)\n",
    "    processed_in = in_time.apply(pd.to_datetime, errors='coerce')\n",
    "    processed_out = out_time.apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "    # Calcul de la durée de travail journalière\n",
    "    duration = processed_out - processed_in\n",
    "    # Conversion en heures\n",
    "    duration_hours = duration.apply(lambda x: x.dt.total_seconds() / 3600)\n",
    "\n",
    "    # Création des nouvelles métriques\n",
    "    # Moyenne des heures travaillées par jour (en ignorant les NaN/absences)\n",
    "    mean_hours = duration_hours.mean(axis=1)\n",
    "    # Nombre total de jours travaillés dans l'année\n",
    "    working_days = duration_hours.count(axis=1)\n",
    "\n",
    "    # Création du DataFrame des features temporelles\n",
    "    time_features = pd.DataFrame({\n",
    "        'EmployeeID': in_time.index,\n",
    "        'AverageWorkingHours': mean_hours.values,\n",
    "        'TotalWorkingDays': working_days.values\n",
    "    })\n",
    "\n",
    "    # 4. Fusion finale\n",
    "    final_df = df_main.merge(time_features, on='EmployeeID', how='left')\n",
    "\n",
    "    # 5. Nettoyage\n",
    "    # Suppression des colonnes qui n'ont qu'une seule valeur (inutiles pour le modèle)\n",
    "    # Exemple: 'Over18' est 'Y' pour tout le monde, 'StandardHours' est 8 pour tout le monde.\n",
    "    nunique = final_df.apply(pd.Series.nunique)\n",
    "    cols_to_drop = nunique[nunique == 1].index\n",
    "    final_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    print(f\"Colonnes supprimées (valeur constante) : {list(cols_to_drop)}\")\n",
    "    print(f\"Taille finale du dataset : {final_df.shape}\")\n",
    "\n",
    "    # 6. Export\n",
    "    final_df.to_csv('data/processed_hr_data.csv', index=False)\n",
    "    print(\"Fichier 'processed_hr_data.csv' généré avec succès.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Exécution\n",
    "if __name__ == \"__main__\":\n",
    "    df = process_hr_data()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723817da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Chargement des données\n",
    "df = pd.read_csv('data/processed_hr_data.csv')\n",
    "\n",
    "# 2. Nettoyage initial\n",
    "if 'EmployeeID' in df.columns:\n",
    "    df = df.drop(columns=['EmployeeID'])\n",
    "\n",
    "# Remplissage des valeurs manquantes (uniquement sur les colonnes numériques)\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "# 3. Traitement de la cible (Target)\n",
    "if 'Attrition' in df.columns:\n",
    "    df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 4. Traitement des variables catégorielles\n",
    "\n",
    "# A) Variable Ordinale (Ordre important) : BusinessTravel\n",
    "# On map manuellement pour respecter la hiérarchie : Non < Rare < Frequent\n",
    "travel_map = {'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2}\n",
    "if 'BusinessTravel' in df.columns:\n",
    "    df['BusinessTravel'] = df['BusinessTravel'].map(travel_map)\n",
    "\n",
    "# B) Variables Nominales (Pas d'ordre) : One-Hot Encoding\n",
    "# Cela va créer des colonnes comme 'Department_Sales', 'Department_HR', etc.\n",
    "nominal_cols = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
    "# On vérifie que les colonnes existent bien\n",
    "nominal_cols = [c for c in nominal_cols if c in df.columns]\n",
    "\n",
    "# On applique le One-Hot Encoding (pd.get_dummies)\n",
    "df_encoded = pd.get_dummies(df, columns=nominal_cols)\n",
    "# On convertit tout en nombres (0.0 / 1.0) pour être sûr\n",
    "df_encoded = df_encoded.astype(float)\n",
    "\n",
    "# --- GÉNÉRATION DES DEUX FICHIERS ---\n",
    "\n",
    "# FICHIER 1 : Non Normalisé (Brut mais encodé)\n",
    "# Utile pour Random Forest, XGBoost, interprétation métier.\n",
    "# Les salaires restent à 50000, l'âge à 30, etc.\n",
    "df_not_normalized = df_encoded.copy()\n",
    "df_not_normalized.to_csv('data/processed_hr_data_encoded_raw.csv', index=False)\n",
    "\n",
    "# FICHIER 2 : Normalisé (0-1)\n",
    "# Utile pour Réseaux de Neurones, KNN, Régression Logistique.\n",
    "# Toutes les valeurs sont ramenées entre 0 et 1.\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "df_normalized.to_csv('data/processed_hr_data_encoded_normalized.csv', index=False)\n",
    "\n",
    "print(\"Traitement terminé.\")\n",
    "print(f\"Colonnes générées ({len(df_encoded.columns)}) :\")\n",
    "print(df_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_show():\n",
    "    # 1. Chargement des données encodées (mais non normalisées)\n",
    "    df = pd.read_csv('data/processed_hr_data_encoded_raw.csv')\n",
    "    \n",
    "    # --- PRÉPARATION POUR L'AFFICHAGE ---\n",
    "    # Pour que les graphiques soient lisibles, on recrée temporairement des labels\n",
    "    \n",
    "    # 1. On recrée une colonne \"Attrition_Label\" (0 -> No, 1 -> Yes)\n",
    "    df['Attrition_Label'] = df['Attrition'].map({0: 'No', 1: 'Yes'})\n",
    "    \n",
    "    # 2. On reconstruit la colonne \"JobRole\" à partir des colonnes One-Hot (JobRole_Sales, etc.)\n",
    "    # On prend toutes les colonnes qui commencent par 'JobRole_'\n",
    "    job_role_cols = [c for c in df.columns if c.startswith('JobRole_')]\n",
    "    # On trouve pour chaque ligne quelle colonne a la valeur 1\n",
    "    # .idxmax() renvoie le nom de la colonne (ex: \"JobRole_Manager\")\n",
    "    # .str.replace() enlève le préfixe pour avoir juste \"Manager\"\n",
    "    df['JobRole_Reconstructed'] = df[job_role_cols].idxmax(axis=1).str.replace('JobRole_', '')\n",
    "\n",
    "    # Configuration du style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # --- GRAPHIQUE 1 : Distribution Globale ---\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # On utilise la colonne Label pour avoir \"Yes/No\" sur le graphe\n",
    "    ax = sns.countplot(x='Attrition_Label', data=df, palette='viridis', order=['No', 'Yes'])\n",
    "    plt.title('Répartition des Départs (Attrition)')\n",
    "    plt.xlabel('Départ')\n",
    "    plt.ylabel('Nombre d\\'employés')\n",
    "    \n",
    "    # Pourcentages\n",
    "    total = len(df)\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width()/2\n",
    "        y = p.get_height()\n",
    "        ax.annotate(percentage, (x, y), ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAPHIQUE 2 : Départ par Métier ---\n",
    "    # On utilise la colonne JobRole reconstruite et la valeur numérique Attrition (0/1) pour la moyenne\n",
    "    role_attrition = df.groupby('JobRole_Reconstructed')['Attrition'].mean() * 100\n",
    "    role_attrition = role_attrition.sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=role_attrition.index, y=role_attrition.values, palette='magma')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Taux de Départ par Métier (%)')\n",
    "    plt.ylabel('% de départ')\n",
    "    plt.xlabel('Métier')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAPHIQUE 3 : Heures de Travail ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x='Attrition_Label', y='AverageWorkingHours', data=df, palette='coolwarm', order=['No', 'Yes'])\n",
    "    plt.title('Comparaison des Heures de Travail Moyennes')\n",
    "    plt.ylabel('Heures / Jour')\n",
    "    plt.xlabel('Départ')\n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAPHIQUE 4 : Ancienneté ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # On filtre sur 1 et 0\n",
    "    sns.kdeplot(data=df[df['Attrition'] == 1]['YearsAtCompany'], label='Départ (Yes)', fill=True, color='red', alpha=0.3)\n",
    "    sns.kdeplot(data=df[df['Attrition'] == 0]['YearsAtCompany'], label='Reste (No)', fill=True, color='blue', alpha=0.3)\n",
    "    plt.title(\"Distribution de l'Ancienneté\")\n",
    "    plt.xlabel('Années dans l\\'entreprise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAPHIQUE 5 : Matrice de Corrélation ---\n",
    "    # Ces colonnes numériques existent toujours telles quelles\n",
    "    cols = ['Age', 'MonthlyIncome', 'AverageWorkingHours', 'YearsAtCompany', \n",
    "            'DistanceFromHome', 'NumCompaniesWorked', 'TrainingTimesLastYear']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # On calcule la corrélation sur tout le dataframe ou juste ces colonnes\n",
    "    corr = df[cols].corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "    plt.title('Matrice de Corrélation (Variables Clés)')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_and_show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
